<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="UniFusion proposes a framework which uses VLMs as unified encoders for image generation and editing.">
  <meta name="keywords" content="vision-language model, VLM, diffusion transformer, flow matching, image generation, image editing, unified models, unifusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>UniFusion</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" type="image/png" href="./static/images/adobe_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-full-width">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <!-- <span style="font-weight: 900;">UniFusion</span><br><span style="font-weight: 100;">Vision-Language Model as Unified Encoder for Image Generation</span> -->
            <span style="font-weight: 900;">UniFusion</span>
          </h1>
          <h2 class="title is-2 publication-title">
            <span style="font-weight: 100;">Vision-Language Model as Unified Encoder for Image Generation</span>
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://thekevinli.github.io">Kevin (Yu-Teng) Li</a>*
            </span>
            <span class="author-block">
              <a href="">Manuel Brack</a>*
            </span>
            <span class="author-block">
              <a href="">Sudeep Katakol</a>
            </span>
            <span class="author-block">
              <a href="">Hareesh Ravi</a>
            </span>
            <span class="author-block">
              <a href="">Ajinkya Kale</a>
            </span>
            <!-- <p style="font-size: smaller;">*Equal contributions</p> -->
            <p style="font-size: .9rem; margin-top: 4px; margin-bottom: 4px;">* Equal contributions. Kevin led the model design and ablations. Manuel led the evaluation and paper writing.</p>
          </div>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">Adobe</span> -->
            <span class="author-block">
              <img src="./static/images/adobe_wordmark_rgb_red.png" width="100px" style="margin: 10px"/>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!--
              <span class="link-block">
                <a href="static/files/textured_gaussians_main.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            -->
              <!-- Arxiv Link. -->
              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.12789"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>ArXiv</span>
                </a>
              </span>
              
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=ttDCDbhEf7Q"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-google-drive"></i>
                  </span>
                  <span>High-res paper</span>
                </a>
              </span>
              
              <!--
              <span class="link-block">
                <a href=""
                   class="button is-normal is-rounded is-dark">
                  <span>Results</span>
                  </a>
              </span>
              -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="teaser-video-container">
        <div class="teaser-video-wrapper" id="teaser-video-wrapper">
          <img id="teaser-thumbnail" src="./static/images/unifusion_arch.png" alt="UniFusion Architecture" />
        </div>
      </div>
      <!-- <p class="has-text-centered is-7" style="margin-top: 5px; margin-bottom: 5px;"><strong>UniFusion Architecture</strong></p> -->
      <h2 class="subtitle has-text-centered">
        <strong>UniFusion</strong> is the first architecture that uses only VLM as input-condition encoder without auxiliary signals from VAE or CLIP to do image editing with competitive quality. The unified encoder framework and our proposed Layerwise Attention Pooling (LAP) module enables emergent capabilities such as <strong>zero-shot multi-reference generation</strong> when trained on single-reference pairs, and capability transfer where training on Editing helps T2I quantitatively and qualitatively.
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <!-- Text-to-Image Results -->
  <div class="container">
    <h2 class="title is-3 has-text-centered">Text-to-Image Results</h2>
    <div class="collage" id="image-collage">
      <a class="collage-item wide-portrait" href="./static/images/text_to_image/portrait_unifusion_zeroshot_768x1080.png">
        <img src="./static/images/text_to_image/portrait_unifusion_zeroshot_768x1080.png" alt="portrait_unifusion_zero_shot" loading="lazy">
      </a>
      <a class="collage-item landscape" href="./static/images/text_to_image/landscape_18.png">
        <img src="./static/images/text_to_image/landscape_18.png" alt="landscape_18" loading="lazy">
      </a>
      <!-- <a class="collage-item landscape" href="./static/images/text_to_image/landscape_0.png">
        <img src="./static/images/text_to_image/landscape_0.png" alt="landscape_0" loading="lazy">
      </a> -->
      <a class="collage-item landscape" href="./static/images/text_to_image/landscape_1.png">
        <img src="./static/images/text_to_image/landscape_1.png" alt="landscape_1" loading="lazy">
      </a>
      <a class="collage-item landscape" href="./static/images/text_to_image/landscape_8.png">
        <img src="./static/images/text_to_image/landscape_8.png" alt="landscape_8" loading="lazy">
      </a>
      <a class="collage-item landscape" href="./static/images/text_to_image/landscape_11.png">
        <img src="./static/images/text_to_image/landscape_11.png" alt="landscape_11" loading="lazy">
      </a>
      <a class="collage-item landscape" href="./static/images/text_to_image/landscape_13.png">
        <img src="./static/images/text_to_image/landscape_13.png" alt="landscape_13" loading="lazy">
      </a>
      <a class="collage-item landscape" href="./static/images/text_to_image/landscape_14.png">
        <img src="./static/images/text_to_image/landscape_14.png" alt="landscape_14" loading="lazy">
      </a>
      <a class="collage-item portrait" href="./static/images/text_to_image/portrait_3_512x720.png">
        <img src="./static/images/text_to_image/portrait_3_512x720.png" alt="portrait_3" loading="lazy">
      </a>
      <!-- <a class="collage-item portrait" href="./static/images/text_to_image/portrait_1.png">
        <img src="./static/images/text_to_image/portrait_1.png" alt="portrait_1" loading="lazy">
      </a> -->
      <a class="collage-item portrait" href="./static/images/text_to_image/portrait_18.png">
        <img src="./static/images/text_to_image/portrait_18.png" alt="portrait_18" loading="lazy">
      </a>
      <!-- <a class="collage-item portrait" href="./static/images/text_to_image/portrait_7.png">
        <img src="./static/images/text_to_image/portrait_7.png" alt="portrait_7" loading="lazy">
      </a> -->
      <a class="collage-item landscape" href="./static/images/text_to_image/landscape_2.png">
        <img src="./static/images/text_to_image/landscape_2.png" alt="landscape_2" loading="lazy">
      </a>
      <a class="collage-item landscape" href="./static/images/text_to_image/landscape_20.png">
        <img src="./static/images/text_to_image/landscape_20.png" alt="landscape_20" loading="lazy">
      </a>
    </div>
  </div>
</section>


<section class="section">
  <!-- Text-to-Image Results -->
  <div class="container">
    <h2 class="title is-3 has-text-centered">Single-reference editing results</h2>
    <h2 class="subtitle is-5 has-text-centered" style="padding-top: 10px;">
      Hover to see the full edited image and instruction prompts.
    </h2>
    <div class="collage collage--editing" id="image-collage-editing">
    <!-- <div class="collage" id="image-collage"> -->
      <a class="collage-item portrait" href="./static/images/single_ref_editing/portrait_005_character_repose_cropped.png">
        <img src="./static/images/single_ref_editing/portrait_005_character_repose_cropped.png" alt="portrait_005_character_repose" loading="lazy">
        <div class="prompt-overlay">a photo of this cat swimming in a river in a jungle, go pro action footage</div>
      </a>
      <a class="collage-item portrait" href="./static/images/single_ref_editing/portrait_001_color_change.png">
        <img src="./static/images/single_ref_editing/portrait_001_color_change.png" alt="portrait_001_color_change" loading="lazy">
        <div class="prompt-overlay">change the color to the wall of the house to yellow</div>
      </a>
      <a class="collage-item landscape" href="./static/images/single_ref_editing/landscape_001_text_editing.png">
        <img src="./static/images/single_ref_editing/landscape_001_text_editing.png" alt="landscape_001_text_editing" loading="lazy">
        <div class="prompt-overlay">change the words to say "watch out above you"</div>
      </a>
      <a class="collage-item landscape" href="./static/images/single_ref_editing/landscape_002_character_repose.png">
        <img src="./static/images/single_ref_editing/landscape_002_character_repose.png" alt="landscape_002_character_repose" loading="lazy">
        <div class="prompt-overlay">Using the reference portrait as identity, generate a new image of the same woman throwing pottery at a wheel in a rustic studio. Keep facial features, hair color/part, approximate age, and skin texture consistent. Hands should be clay-smeared; add soft daylight from a high window plus a warm rim light from camera right. 35 mm, f/2.0, chest-level angle</div>
      </a>
      <a class="collage-item square" href="./static/images/single_ref_editing/square_001_expression_change.png">
        <img src="./static/images/single_ref_editing/square_001_expression_change.png" alt="square_001_expression_change" loading="lazy">
        <div class="prompt-overlay">change this character to smiling</div>
      </a>
      <a class="collage-item portrait" href="./static/images/single_ref_editing/portrait_002_character_repose.png">
        <img src="./static/images/single_ref_editing/portrait_002_character_repose.png" alt="portrait_002_character_repose" loading="lazy">
        <div class="prompt-overlay">Generate an image of this photo showing the whole body of the subject. The subject is wearing a fluffy white headband.</div>
      </a>
      <a class="collage-item landscape" href="./static/images/single_ref_editing/landscape_003_text_editing.png">
        <img src="./static/images/single_ref_editing/landscape_003_text_editing.png" alt="landscape_003_text_editing" loading="lazy">
        <div class="prompt-overlay">change the "STOP" to "START"</div>
      </a>
      <a class="collage-item portrait" href="./static/images/single_ref_editing/portrait_003_object_extraction.png">
        <img src="./static/images/single_ref_editing/portrait_003_object_extraction.png" alt="portrait_003_object_extraction" loading="lazy">
        <div class="prompt-overlay">Isolate only the rosemary on yellow background</div>
      </a>
      <a class="collage-item landscape" href="./static/images/single_ref_editing/landscape_004_stylization.png">
        <img src="./static/images/single_ref_editing/landscape_004_stylization.png" alt="landscape_004_stylization" loading="lazy">
        <div class="prompt-overlay">Using this style make some art of a boat in the harbor of an old town</div>
      </a>
      <a class="collage-item portrait" href="./static/images/single_ref_editing/portrait_004_character_repose.png">
        <img src="./static/images/single_ref_editing/portrait_004_character_repose.png" alt="portrait_004_character_repose" loading="lazy">
        <div class="prompt-overlay">Same woman in a modern lab wearing a white coat and nitrile gloves, holding a pipette near a rack; match facial structure and hair color; neutral white balance.</div>
      </a>
      <a class="collage-item landscape" href="./static/images/single_ref_editing/portrait_006_extraction_change_ar.png">
        <img src="./static/images/single_ref_editing/portrait_006_extraction_change_ar.png" alt="portrait_006_extraction_change_ar" loading="lazy">
        <div class="prompt-overlay">Place the dog on a white background</div>
      </a>
      <a class="collage-item landscape" href="./static/images/single_ref_editing/landscape_005_extraction_change_ar.png">
        <img src="./static/images/single_ref_editing/landscape_005_extraction_change_ar.png" alt="landscape_005_extraction_change_ar" loading="lazy">
        <div class="prompt-overlay">add earrings to a white marble round stand, add delicate shadows in the background of the flowers, place a white rose next to the stand</div>
      </a>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="carousel-label" class="column is-full-width">
        <div class="column has-text-centered">
          <h2 class="title is-3 has-text-centered">Zero-shot multi-reference generations</h2>
        </div>
      </div>
      <div id="carousel-div" class="carousel results-carousel" data-autoplay="true" data-pagination="true" data-navigation="true" data-delay="3000">
        <div class="item"><img src="./static/images/multi_ref_editing/three_ref_001.png" alt="carousel placeholder 1"></div>
        <div class="item"><img src="./static/images/multi_ref_editing/three_ref_002.png" alt="carousel placeholder 2"></div>
        <div class="item"><img src="./static/images/multi_ref_editing/two_ref_001.png" alt="carousel placeholder 3"></div>
        <div class="item"><img src="./static/images/multi_ref_editing/two_ref_002.png" alt="carousel placeholder 4"></div>
        <div class="item"><img src="./static/images/multi_ref_editing/two_ref_003.png" alt="carousel placeholder 5"></div>
        <div class="item"><img src="./static/images/multi_ref_editing/two_ref_004.png" alt="carousel placeholder 6"></div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <p class="content has-text-justified" style="margin-top: 15px;">
        <span>Results shown are generated by a UniFusion checkpoint trained for roughly 11k steps on editing and has <em>never</em> seen multi-reference pairs.</span>
        <span><strong>Hover over the image to pause the carousel!</strong></span>
      </p>
    </div>
  </div>
</section>


<section class="section fullwidth-gray-bg">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <!-- We present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images (e.g., Variational Autoencoder i.e. VAE features) and text (e.g., T5 or CLIP). This separation constrains diffusion models’ ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often perform shallow fusion of information from VLM without enough generalization, employ multiple visual encoders, or train large unified VLMs jointly for text and image generation—approaches that demand substantial computational resources and large-scale data, limiting accessibility. To reap the benefits of the joint multimodal reasoning and representation capacity of VLMs, we propose UniFusion.  -->

            Despite rapid advancements in visual generative models, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models’ ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large multimodal unified models which demands large-scale computation resources and datas.
          </p>
          <p>
            To maximize the benefits of the joint multimodal reasoning and representation capacity of VLMs, we present <strong>UniFusion</strong>, a novel framework for image generation using frozen VLMs as unified encoders. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high-level semantics and low-level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithfully transfer of visual information from a VLM to the diffusion model to empower high-quality editing.
          </p>
          <p>
            <!-- We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference.  -->
            With an 8B VLM and an 8B DiT, UniFusion surpasses Flux.1 [dev] and BAGEL on DPG-Bench with a smaller training set, while comparing favorably against Flux.1 Kontext [dev] and Qwen-Image-Edit in editing tasks without any post-training. We further discovered that editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities -- the UniFusion checkpoint trained on single-image reference could generalize zero-shot to multi-reference inputs, further motivating the unified encoder design of UniFusion.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
<!-- Model Pipeline. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Conditioning Architectures</h2>
    <div class="content has-text-centered">
      <img src="./static/images/overview_with_vqa_score.png" style="width: 100%;">
    </div>
    <div class="content has-text-justified">
      <p>
        We begin by ablating four conditioning strategies — including the conventional last-layer, key-value fusion, and hidden-state injection — and find that Layerwise Attention Pooling (LAP) consistently performs best across tasks. However, LAP alone is not a simple drop-in replacement to surpass T5, especially when the base model (e.g., Llama 3.1-8B) contains insufficient capacity. By switching to InternVL 2.5-8B in later section, which provides richer joint representations, we match and even exceed T5 performance once paired with our <strong>VeriFi</strong> rewriting mechanism, achieving both stronger prompt alignment and improved visual grounding.
      </p>
    </div>
  </div>
</div>
</section>


<section class="section">
  <div class="container is-max-desktop">
<!-- Model Pipeline. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">UniFusion Design Choices</h2>
    <div class="content has-text-centered">
      <img src="./static/images/lap_visualization_qk_cluster.png" style="width: 100%;">
    </div>
    <div class="content has-text-justified">
      <p>
        One of the motivations for LAP was high-level semantics and low-level concepts are dispersed across different layers, all of which are useful for the downstream image generation model. Our intuition was confirmed as we visualize query-key activation norms in a trained LAP module, where different tokens activate different VLM layers, and on many tokens, the model utilizes implicit clusters of adjacent layers. Given this observation, along with VLM layers cosine-similarity plot (shown in paper), we decided to subsample every N layers (e.g. N=3) from the VLM as input to LAP in the final UniFusion architecture, so as to reduce memory overhead and mitigate the chance of model overfitting to certain local cluster of layers and fail to incorporate all useful information. 
        <!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas tincidunt auctor tortor non viverra. Phasellus elit arcu, iaculis eget bibendum id, ultricies sed risus. Etiam ut est a nisl hendrerit vehicula euismod in purus. Vestibulum et dapibus est. Ut in mauris tempor justo tincidunt congue sit amet quis ipsum. Etiam dapibus nisi risus, ut ornare velit suscipit in. Cras consectetur rutrum magna, et rutrum velit pharetra fermentum. Aenean pulvinar accumsan justo quis scelerisque. Vivamus varius in diam in ullamcorper. Vestibulum porta elementum dolor vel lacinia. -->
      </p>
      <div class="content has-text-centered">
        <img src="./static/images/layer_dropout.png" style="width: 100%;">
      </div>
      <p>
        In addition to activation analysis, we did a layer dropout study before LAP to understand the influence of each VLM layer on the visual output. We observe that image generation does not strongly rely on the first and last layers. When zeroing out the respective weights during pooling, the image composition remains mostly unchanged. Interestingly, dropping out the middle layers breaks the prompt understanding but does not generate complete noise. Instead, early / late layers retained low-level concepts like "mountain" in the example of the top row.
      </p>
      <div class="content has-text-centered">
        <img src="./static/images/image_recon_metrics.png" style="width: 100%;">
      </div>
      <p>
        Can VLM layers reconstruct images? To answer this question, we included a small percentage of image reconstruction batches (roughly 3%) from VLM encoded images tiles, during text-to-image training stage of UniFusion. The result indicates that not only can VLM layers reconstruct images, there exists a clear scaling trend as the number of <em>tiles</em> increases - global structure can be captured with just 1 thumbnail tile, while fine-grained details require more tiles to retrain. Interestingly, even when UniFusion is <em>not trained</em> with image reconstruction batches in early experiments, at test-time when a VLM-encoded image is provided, the downstream DiT generates visually similar images.  
        <!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas tincidunt auctor tortor non viverra. Phasellus elit arcu, iaculis eget bibendum id, ultricies sed risus. Etiam ut est a nisl hendrerit vehicula euismod in purus. Vestibulum et dapibus est. Ut in mauris tempor justo tincidunt congue sit amet quis ipsum. Etiam dapibus nisi risus, ut ornare velit suscipit in. Cras consectetur rutrum magna, et rutrum velit pharetra fermentum. Aenean pulvinar accumsan justo quis scelerisque. Vivamus varius in diam in ullamcorper. Vestibulum porta elementum dolor vel lacinia. -->
      </p>
      <div class="content has-text-centered">
        <img src="./static/images/verifi.png" style="width: 100%;">
      </div>
      <p>
        UniFusion intentionally does not train with user input prompt for most batches. Instead, we propose <strong>VLM-Enabled Rewriting Injection with Flexible Inference</strong> (VeriFi), which comes with 3 significant benefits:
        <ul>
          <li>Different from standalone rewriter models, VeriFi perform a single forward pass without re-encoding, which means the decoded text still attends to image and text inputs directly, aligning features between modalities better and grounding potential hallucination. </li>
          <li> Repetition of important tokens from the original prompt further mitigates "position biases" in causal attention - later tokens have less activation products. </li>
          <li>While using the same system prompt during training, we can meaningfullly influence the model's behavior by adopting a different system prompt during inference.</li>
        </ul>
      </p>
      <div class="content has-text-centered">
        <img src="./static/images/t2i_dpg_bench.png" style="width: 80%;">
      </div>
      <p>
        <strong>UniFusion</strong> achieves competitive performance on DPG-Bench against much larger models trained on more data. We report average and best generation across four seeds at 1024px resolution. Macro Average is taken as mean over scores per category, whereas Micro averages scores across all prompts. Results are scored by Gemma-3-27B with extensive CoT to reduce hallucinations in scoring.
      </p>
      <div class="content has-text-centered">
        <img src="./static/images/editing_comparison.png" style="width: 100%;">
      </div>
    </div>
  </div>
</div>
</section>


<section class="section fullwidth-gray-bg">
  <div class="container is-max-desktop">
<!-- Model Pipeline. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Capability Transfer - Editing helps text-to-image generation</h2>
    <div class="content has-text-centered">
      <img src="./static/images/capability_transfer.png" style="width: 100%;">
    </div>
    <div class="content has-text-justified">
      <p>
        UniFusion demonstrated significant improvements in prompt alignment for text-to-image generation after finetuning on editing tasks for just <em>11k steps</em>. Specifically, UniFusion-Edit leads UniFusion-Base by over 2 percentage points in Micro Avg of DPG-Bench. In an A/B test conducted on  180 annotators with 616 prompts, 2 seeds each, UniFusion-Edit obtains almost double the win rate of UniFusion-Base.
      </p>
      <p>
        We hypothesize that this behavior is a direct benefit of the unified encoder framework. In a unified encoder space, a single concept represented by different modalities should be more aligned, compared to standard practice of VAE vs T5 space. For DiT, this suggest two theoretical benefits - first, less disruption in parameters space during the transition from text-to-image towards editing tasks; second, by optimizing the same flow matching (or denoising) loss given different modalities of the same concept, DiT is able to put together a more complete understanding of the concept, subsequently transferring the learning from one task to another.
      </p>
    </div>
  </div>
</div>
</section>


<footer class="footer" class="fullwidth-gray-bg">
  <div class="container">
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
  @article{li2025unifusion,
    author={Li, Yu-Teng and Brack, Manuel and Katakol, Sudeep and Ravi, Hareesh and Kale, Ajinkya},
    title={UniFusion: A Unified Encoder for Zero-shot Multi-reference Text-to-Image Generation},
    journal={arXiv preprint arXiv:2505.06582},
    year={2025},
  }
        </code></pre>
      </div>
    </section>
    <div class="content has-text-centered">
      
      <a class="icon-link"
         href="https://arxiv.org/abs/2510.12789">
        <i class="ai ai-arxiv"></i>
      </a>
    
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
      <br>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
